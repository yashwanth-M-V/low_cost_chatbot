import os
from typing import Optional
from llama_cpp import Llama
from dotenv import load_dotenv
import logging

load_dotenv()
MODEL_PATH = os.getenv("MISTRAL_MODEL_PATH")

logging.basicConfig(level=logging.WARNING)

def load_mistral_model(
    model_path: str = MODEL_PATH,
    n_ctx: int = int(os.getenv("MODEL_CTX", "512")),
    n_gpu_layers: int = int(os.getenv("GPU_LAYERS", "35")),
    n_threads: int = int(os.getenv("THREADS", "6")),
    n_threads_batch: int = int(os.getenv("THREADS_BATCH", "6")),
    offload_kqv: bool = os.getenv("OFFLOAD_KQV", "True") == "True",
    use_mmap: bool = os.getenv("USE_MMAP", "False") == "True",  # Safer default
    use_mlock: bool = os.getenv("USE_MLOCK", "False") == "True",
    vocab_only: bool = False,
    flash_attn: bool = os.getenv("FLASH_ATTN", "True") == "True",
    verbose: bool = False,
    log_handler: Optional[logging.Handler] = None
) -> Optional[Llama]:
    """Loads Mistral model with environment-configured parameters"""
    if not os.path.exists(model_path):
        logging.error(f"Model file not found: {model_path}")
        return None

    try:
        return Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            n_threads=n_threads,
            n_threads_batch=n_threads_batch,
            offload_kqv=offload_kqv,
            use_mmap=use_mmap,
            use_mlock=use_mlock,
            vocab_only=vocab_only,
            flash_attn=flash_attn,
            verbose=verbose,
            log_handler=log_handler
        )
    except Exception as e:
        logging.error(f"Model load error: {str(e)}", exc_info=True)
        return None